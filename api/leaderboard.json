[
  {
    "rank": 1,
    "modelName": "davidkim205/Rhea-72b-v0.5",
    "change": "same",
    "average": 81.22,
    "arc": 79.78,
    "hellaSwag": 91.15,
    "mmlu": 77.95,
    "truthfulQA": 74.5,
    "winogrande": 87.85,
    "gsm8k": 76.12,
    "usage": 1384193
  },
  {
    "rank": 2,
    "modelName": "MTSAIR/MultiVerse_70B",
    "change": "up",
    "average": 81.0,
    "arc": 78.67,
    "hellaSwag": 89.77,
    "mmlu": 78.22,
    "truthfulQA": 75.18,
    "winogrande": 87.53,
    "gsm8k": 76.65,
    "usage": 1319156
  },
  {
    "rank": 3,
    "modelName": "SF-Foundation/Ein-72B-v0.11",
    "change": "same",
    "average": 80.81,
    "arc": 76.79,
    "hellaSwag": 89.02,
    "mmlu": 77.2,
    "truthfulQA": 79.02,
    "winogrande": 84.06,
    "gsm8k": 78.77,
    "usage": 1298529
  },
  {
    "rank": 4,
    "modelName": "abacusai/Smaug-72B-v0.1",
    "change": "up",
    "average": 80.48,
    "arc": 76.02,
    "hellaSwag": 89.27,
    "mmlu": 77.15,
    "truthfulQA": 76.67,
    "winogrande": 85.08,
    "gsm8k": 78.7,
    "usage": 1255720
  },
  {
    "rank": 5,
    "modelName": "ibivibivi/alpaca-dragon-72b-v1",
    "change": "down",
    "average": 79.3,
    "arc": 73.89,
    "hellaSwag": 88.16,
    "mmlu": 77.4,
    "truthfulQA": 72.69,
    "winogrande": 86.03,
    "gsm8k": 77.63,
    "usage": 1239050
  },
  {
    "rank": 6,
    "modelName": "mistralai/Mixtral-8x22B-Instruct-v0.1",
    "change": "same",
    "average": 79.15,
    "arc": 72.7,
    "hellaSwag": 89.08,
    "mmlu": 77.77,
    "truthfulQA": 68.14,
    "winogrande": 85.16,
    "gsm8k": 82.03,
    "usage": 1208482
  },
  {
    "rank": 7,
    "modelName": "moreh/MoMo-72B-lora-1.8.7-DPO",
    "change": "up",
    "average": 78.55,
    "arc": 70.82,
    "hellaSwag": 85.96,
    "mmlu": 77.13,
    "truthfulQA": 74.71,
    "winogrande": 84.06,
    "gsm8k": 78.62,
    "usage": 1205577
  },
  {
    "rank": 8,
    "modelName": "cloudyu/TomGrc_FusionNet_34B2_MoE",
    "change": "up",
    "average": 77.91,
    "arc": 74.06,
    "hellaSwag": 86.74,
    "mmlu": 76.65,
    "truthfulQA": 72.24,
    "winogrande": 83.35,
    "gsm8k": 74.45,
    "usage": 1199621
  },
  {
    "rank": 9,
    "modelName": "meta-llama/Meta-Llama-3-70B-instruct",
    "change": "down",
    "average": 77.88,
    "arc": 71.42,
    "hellaSwag": 85.69,
    "mmlu": 80.06,
    "truthfulQA": 61.81,
    "winogrande": 82.87,
    "gsm8k": 85.44,
    "usage": 1195080
  },
  {
    "rank": 10,
    "modelName": "satluf/luxia-21-4b-alignment-v1.0",
    "change": "same",
    "average": 77.74,
    "arc": 77.47,
    "hellaSwag": 91.88,
    "mmlu": 68.1,
    "truthfulQA": 79.17,
    "winogrande": 87.37,
    "gsm8k": 71.11,
    "usage": 1185234
  },
  {
    "rank": 11,
    "modelName": "zhengjr/MixTAO-7B2-MoE-v8.1",
    "change": "up",
    "average": 77.5,
    "arc": 73.81,
    "hellaSwag": 89.22,
    "mmlu": 64.92,
    "truthfulQA": 78.57,
    "winogrande": 87.37,
    "gsm8k": 71.11,
    "usage": 1178089
  },
  {
    "rank": 12,
    "modelName": "yuncongong/Truthful_DPO_TomGrc_Fusi...",
    "change": "down",
    "average": 77.44,
    "arc": 74.91,
    "hellaSwag": 89.3,
    "mmlu": 64.67,
    "truthfulQA": 78.02,
    "winogrande": 88.24,
    "gsm8k": 69.52,
    "usage": 1177065
  },
  {
    "rank": 13,
    "modelName": "JaeyeonKang/CCK_Asura_v1",
    "change": "same",
    "average": 77.43,
    "arc": 73.89,
    "hellaSwag": 89.07,
    "mmlu": 75.44,
    "truthfulQA": 71.75,
    "winogrande": 88.35,
    "gsm8k": 68.08,
    "usage": 1170954
  },
  {
    "rank": 14,
    "modelName": "fbliq/UNA-SimpleSmaug-34b-vbeta",
    "change": "down",
    "average": 77.41,
    "arc": 74.57,
    "hellaSwag": 86.74,
    "mmlu": 76.68,
    "truthfulQA": 70.17,
    "winogrande": 88.82,
    "gsm8k": 72.48,
    "usage": 1162959
  },
  {
    "rank": 15,
    "modelName": "TomGrc/FusionNet_34Bx2_MoE_v0.1",
    "change": "same",
    "average": 77.38,
    "arc": 74.72,
    "hellaSwag": 86.46,
    "mmlu": 76.72,
    "truthfulQA": 71.01,
    "winogrande": 83.05,
    "gsm8k": 73.01,
    "usage": 1144223
  },
  {
    "rank": 16,
    "modelName": "migutisser/Tess-72b-v1.5b",
    "change": "same",
    "average": 77.3,
    "arc": 71.25,
    "hellaSwag": 88.53,
    "mmlu": 81.22,
    "truthfulQA": 71.99,
    "winogrande": 81.45,
    "gsm8k": 76.98,
    "usage": 1129110
  }
]
